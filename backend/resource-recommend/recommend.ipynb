{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import json\n",
    "from pulp import LpProblem, LpMaximize, LpVariable, lpSum, LpStatus, lpSum, LpBinary, PULP_CBC_CMD\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# def load_graph(path):\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# def save_graph(path, graph_data):\n",
    "#     os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(graph_data, f, indent=2)\n",
    "\n",
    "# def extract_prereq_edges(graph):\n",
    "#     return [\n",
    "#         (rel[\"source\"].lower(), rel[\"target\"].lower(), rel[\"weight\"])\n",
    "#         for rel in graph.get(\"relationships\", [])\n",
    "#         if rel[\"relationship\"] == \"prerequisite\"\n",
    "#     ]\n",
    "\n",
    "# def build_skill_set(graph):\n",
    "#     return set(skill[\"id\"].lower() for skill in graph.get(\"skills\", []))\n",
    "\n",
    "# def update_prerequisites_from_folder(folder_path, output_folder=\"updated_skill_graph\"):\n",
    "#     json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "#     graph_paths = [os.path.join(folder_path, f) for f in json_files]\n",
    "\n",
    "#     graphs = {path: load_graph(path) for path in graph_paths}\n",
    "#     skills_by_path = {path: build_skill_set(graphs[path]) for path in graph_paths}\n",
    "#     prereqs_by_path = {path: extract_prereq_edges(graphs[path]) for path in graph_paths}\n",
    "\n",
    "#     all_prereqs = set()\n",
    "#     for prereqs in prereqs_by_path.values():\n",
    "#         all_prereqs.update(prereqs)\n",
    "\n",
    "#     for path in graph_paths:\n",
    "#         current_skills = skills_by_path[path]\n",
    "#         current_rels = graphs[path][\"relationships\"]\n",
    "#         existing_edges = {\n",
    "#             (rel[\"source\"].lower(), rel[\"target\"].lower())\n",
    "#             for rel in current_rels\n",
    "#             if rel[\"relationship\"] == \"prerequisite\"\n",
    "#         }\n",
    "\n",
    "#         # Add missing prerequisite edges\n",
    "#         for src, tgt, weight in all_prereqs:\n",
    "#             if src in current_skills and tgt in current_skills and (src, tgt) not in existing_edges:\n",
    "#                 graphs[path][\"relationships\"].append({\n",
    "#                     \"source\": src,\n",
    "#                     \"target\": tgt,\n",
    "#                     \"relationship\": \"prerequisite\",\n",
    "#                     \"weight\": weight\n",
    "#                 })\n",
    "\n",
    "#         # Save updated graph\n",
    "#         filename = os.path.basename(path)\n",
    "#         output_path = os.path.join(output_folder, filename)\n",
    "#         save_graph(output_path, graphs[path])\n",
    "\n",
    "# update_prerequisites_from_folder(\"../skill_graph\", output_folder=\"updated_skill_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_prereq_map(prereq_graph):\n",
    "    prereq_map = defaultdict(set)\n",
    "    for src, tgt in prereq_graph.edges():\n",
    "        prereq_map[tgt].add(src)\n",
    "    return prereq_map\n",
    "\n",
    "def matchJobDomain(model, job_domains: dict, input_job_title: str, input_skills: list):\n",
    "    \"\"\"\n",
    "    job_domains: dict of { job_domain_name: [skill1, skill2, ...] }\n",
    "    input_job_title: str (e.g., \"Data Analyst\")\n",
    "    input_skills: list of strings OR list of [skill, ...] tuples (only first element used)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize input skills if passed as list of lists (e.g., [[\"Python\", 0.5, 0.6], ...])\n",
    "    if isinstance(input_skills[0], (list, tuple)):\n",
    "        input_skills = [skill[0] for skill in input_skills]\n",
    "\n",
    "    # Encode input job title and skills\n",
    "    input_title_emb = model.encode(input_job_title, convert_to_tensor=True)\n",
    "    input_skill_embs = model.encode(input_skills, convert_to_tensor=True)\n",
    "\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "\n",
    "    for domain_title, domain_skills in job_domains.items():\n",
    "        # Encode domain title\n",
    "        domain_title_emb = model.encode(domain_title, convert_to_tensor=True)\n",
    "        title_score = float(util.cos_sim(input_title_emb, domain_title_emb))\n",
    "\n",
    "        # Encode domain skills\n",
    "        domain_skill_embs = model.encode(domain_skills, convert_to_tensor=True)\n",
    "        sim_matrix = util.cos_sim(input_skill_embs, domain_skill_embs).cpu().numpy()\n",
    "\n",
    "        # For each input skill, get the best match in the domain\n",
    "        best_sim_per_skill = sim_matrix.max(axis=1)\n",
    "        skill_score = best_sim_per_skill.mean() if len(best_sim_per_skill) > 0 else 0.0\n",
    "\n",
    "        combined_score = 0.4 * title_score + 0.6 * skill_score\n",
    "\n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_match = {\n",
    "                \"matched_domain\": domain_title,\n",
    "                \"title_score\": round(title_score, 4),\n",
    "                \"skill_score\": round(skill_score, 4),\n",
    "                \"combined_score\": round(combined_score, 4),\n",
    "            }\n",
    "\n",
    "    return best_match\n",
    "\n",
    "def extract_courses_for_skill(course_df, skill_name):\n",
    "    \"\"\"\n",
    "    Extracts all courses that correspond to the given skill name from the course dataset.\n",
    "    \"\"\"\n",
    "    return course_df[course_df[\"skill\"].str.lower() == skill_name.lower()].copy()\n",
    "\n",
    "def standardize_focus_scores(skill_list):\n",
    "    \"\"\"\n",
    "    Standardizes the focus scores in the skill list so they sum to 1.\n",
    "    Parameters:\n",
    "    - skill_list (list): A list of skills with their focus-score and confidence level.\n",
    "    Returns:\n",
    "    - list: A standardized skill list where focus scores sum to 1.\n",
    "    \"\"\"\n",
    "    # Extract focus scores\n",
    "    focus_scores = np.array([entry[1] for entry in skill_list], dtype=np.float64)\n",
    "\n",
    "    # Normalize so the sum equals 1\n",
    "    total_focus = np.sum(focus_scores)\n",
    "    if total_focus > 0:\n",
    "        normalized_focus_scores = focus_scores / total_focus\n",
    "    else:\n",
    "        normalized_focus_scores = focus_scores  # If total is 0, keep them unchanged\n",
    "\n",
    "    # Update skill list with standardized focus scores\n",
    "    standardized_skill_list = [[skill[0], float(focus), skill[2]] for skill, focus in zip(skill_list, normalized_focus_scores)]\n",
    "\n",
    "    return standardized_skill_list\n",
    "\n",
    "import difflib\n",
    "\n",
    "def compute_match_score(course_title, course_description, main_skill, all_skills, modules, prereq_graph):\n",
    "    \"\"\"\n",
    "    Compute a course's match score based on:\n",
    "    - Title match with main skill\n",
    "    - Mentions of other skills from same module\n",
    "    - Mentions of prerequisites of the main skill\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize course text and skills\n",
    "    title = course_title.lower()\n",
    "    description = course_description.lower()\n",
    "    full_text = f\"{title} {description}\"\n",
    "    all_skill_names = [s[0].lower() if isinstance(s, (list, tuple)) else s.lower() for s in all_skills]\n",
    "    main_skill = main_skill.lower()\n",
    "    prereq_map = build_prereq_map(prereq_graph)\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    # --- 1. Title Matching Boost ---\n",
    "    if main_skill in title:\n",
    "        score += 2.0  # Strong boost for exact match\n",
    "    else:\n",
    "        close_matches = difflib.get_close_matches(main_skill, [title], n=1, cutoff=0.8)\n",
    "        if close_matches:\n",
    "            score += 1.2  # Partial match\n",
    "\n",
    "    # --- 2. Module Group Boost ---\n",
    "    for module in modules:\n",
    "        if main_skill in module[\"skills\"]:\n",
    "            same_module_skills = set(module[\"skills\"]) - {main_skill}\n",
    "            for skill in same_module_skills:\n",
    "                if skill.lower() in full_text:\n",
    "                    score += 0.5\n",
    "                else:\n",
    "                    matches = difflib.get_close_matches(skill.lower(), [full_text], n=1, cutoff=0.8)\n",
    "                    if matches:\n",
    "                        score += 0.3\n",
    "            break\n",
    "\n",
    "    # --- 3. Prerequisite Boost ---\n",
    "    prereqs = prereq_map.get(main_skill, set())\n",
    "    for prereq in prereqs:\n",
    "        if prereq.lower() in full_text:\n",
    "            score += 0.4\n",
    "        else:\n",
    "            matches = difflib.get_close_matches(prereq.lower(), [full_text], n=1, cutoff=0.8)\n",
    "            if matches:\n",
    "                score += 0.2\n",
    "\n",
    "    return round(score, 3)\n",
    "\n",
    "\n",
    "def compute_difficulty_scores(row, main_skill, skill_list):\n",
    "    \"\"\"\n",
    "    Apply difficulty score computation to all courses in the dataset using the confidence level of the main skill.\n",
    "    Parameters:\n",
    "    - course_df (DataFrame): The dataset containing course information.\n",
    "    - main_skill (str): The main skill for which courses are being evaluated.\n",
    "    - skill_list (list): A list of skills with their focus-score and confidence level.\n",
    "    Returns:\n",
    "    - DataFrame: The original DataFrame with an added 'difficulty_score' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to determine the ideal difficulty based on confidence\n",
    "    def get_ideal_difficulty(confidence):\n",
    "        if confidence >= 0.5:\n",
    "            diff = 3\n",
    "        elif confidence >= 0.3:\n",
    "            diff = 2\n",
    "        elif confidence >= 0.2:\n",
    "            diff = 1.5\n",
    "        elif confidence >= 0.1:\n",
    "            diff = 1\n",
    "        else:\n",
    "            diff = 0\n",
    "        return diff\n",
    "\n",
    "    # Function to compute difficulty score for a single course row\n",
    "    def get_difficulty_score(course_difficulty, user_confidence, is_pro_certificate):\n",
    "        ideal_difficulty = get_ideal_difficulty(user_confidence)\n",
    "        difficulty_penalty = 1 * np.abs(ideal_difficulty - course_difficulty)\n",
    "        if is_pro_certificate and user_confidence >= 0.6:\n",
    "            certificate_score = 10\n",
    "        elif is_pro_certificate and user_confidence <= 0.3:\n",
    "            certificate_score = -10\n",
    "        else:\n",
    "            certificate_score = 0\n",
    "        return 1-difficulty_penalty + certificate_score\n",
    "\n",
    "    \n",
    "    main_skill_confidence = next((conf for skill, focus, conf in skill_list if skill.lower() == main_skill.lower()), 0)\n",
    "    return get_difficulty_score(\n",
    "        course_difficulty=row[\"difficulty_numeric\"],\n",
    "        user_confidence=main_skill_confidence,\n",
    "        is_pro_certificate=(row[\"course_type\"] == \"Certificate\")\n",
    "    )\n",
    "\n",
    "# Function to solve the ILP for selecting courses with a dynamic duration constraint\n",
    "def solve_course_selection_pulp(course_df, skill, D_ideal, alpha, beta, lambda_, gamma):\n",
    "    \"\"\"\n",
    "    Solves the ILP for selecting courses that maximize skill match and difficulty score while minimizing price,\n",
    "    while ensuring the total selected duration stays within ±10% of the ideal duration.\n",
    "\n",
    "    Parameters:\n",
    "    - course_df (DataFrame): The dataset containing courses with relevant fields.\n",
    "    - skill (str): The skill being optimized in this iteration.\n",
    "    - skill_list (list): A list of skills with their focus-score and confidence level.\n",
    "    - D_ideal (float): The ideal duration for the selected courses.\n",
    "    - alpha, gamma, lambda_, beta: Weight parameters for optimization.\n",
    "\n",
    "    Returns:\n",
    "    - Selected courses as a list of course indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract relevant courses for the given skill\n",
    "    relevant_courses = course_df[course_df[\"skill\"].str.lower() == skill.lower()].copy()\n",
    "    if relevant_courses.empty:\n",
    "        return []  # No courses available for this skill\n",
    "\n",
    "    # Define decision variables (binary: select or not)\n",
    "    x = {i: LpVariable(f\"x_{i}\", cat=\"Binary\") for i in relevant_courses.index}\n",
    "\n",
    "    # Define the ILP problem\n",
    "    problem = LpProblem(\"Course_Selection\", LpMaximize)\n",
    "\n",
    "    match_score = lpSum(relevant_courses.loc[i, \"match_score\"] * x[i] for i in relevant_courses.index)\n",
    "    difficulty_score = lpSum(relevant_courses.loc[i, \"difficulty_score\"] * x[i] for i in relevant_courses.index)\n",
    "    wilson_score = lpSum(relevant_courses.loc[i, \"wilson_score\"] * x[i] for i in relevant_courses.index)\n",
    "    price_score = lpSum(relevant_courses.loc[i, \"price\"] * x[i] for i in relevant_courses.index)\n",
    "    number_score = lpSum(x[i] for i in relevant_courses.index)\n",
    "\n",
    "    # Objective Function: Maximize average skill match, difficulty, and review score while minimizing price\n",
    "    problem += (\n",
    "        match_score +\n",
    "        alpha * difficulty_score +\n",
    "        beta * wilson_score -\n",
    "        lambda_ * price_score -\n",
    "        gamma *  number_score\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Skill Coverage Constraint: At least one course must be selected\n",
    "    problem += lpSum(x[i] for i in relevant_courses.index) >= 1\n",
    "\n",
    "    # Duration Constraint: Total selected duration must be within ±10% of the ideal duration\n",
    "    duration_tolerance = 0.1 * D_ideal\n",
    "    problem += lpSum(relevant_courses.loc[i, \"duration\"] * x[i] for i in relevant_courses.index) <= (D_ideal + duration_tolerance)\n",
    "    problem += lpSum(relevant_courses.loc[i, \"duration\"] * x[i] for i in relevant_courses.index) >= (D_ideal - duration_tolerance)\n",
    "\n",
    "    # Solve the ILP\n",
    "    problem.solve()\n",
    "\n",
    "    # Extract selected courses\n",
    "    selected_courses = [i for i in relevant_courses.index if x[i].varValue == 1]\n",
    "\n",
    "    return selected_courses\n",
    "\n",
    "def suggest_courses(\n",
    "    embedding_model,\n",
    "    course_df,\n",
    "    skill_list,\n",
    "    total_weeks,\n",
    "    weekly_hours,\n",
    "    job_title,\n",
    "    module_skills,\n",
    "    prereq_graph,\n",
    "    portion=0.8,\n",
    "    alpha=0.5,\n",
    "    beta=0.5,\n",
    "    lambda_=0.5,\n",
    "    gamma=1\n",
    "):\n",
    "    result = {}\n",
    "    skill_list = standardize_focus_scores(skill_list)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Precompute embeddings\n",
    "    skill_embeddings = {\n",
    "        skill[0].lower(): embedding_model.encode(skill[0].lower(), convert_to_tensor=True)\n",
    "        for skill in skill_list\n",
    "    }\n",
    "    job_title_embedding = embedding_model.encode(job_title.lower(), convert_to_tensor=True)\n",
    "\n",
    "    for [skill, focus, confidence] in skill_list:\n",
    "        main_skill = skill\n",
    "        D_ideal = portion * total_weeks * weekly_hours * focus\n",
    "\n",
    "        # Extract + preprocess courses\n",
    "        courses = extract_courses_for_skill(course_df, main_skill)\n",
    "        if courses.empty:\n",
    "            result[main_skill] = []\n",
    "            continue\n",
    "\n",
    "        courses[\"description_embedding\"] = courses[\"description\"].apply(\n",
    "            lambda text: embedding_model.encode(text, convert_to_tensor=True)\n",
    "        )\n",
    "        courses[\"title_embedding\"] = courses[\"title\"].apply(\n",
    "            lambda text: embedding_model.encode(text, convert_to_tensor=True)\n",
    "        )\n",
    "        courses[\"match_score\"] = courses.apply(\n",
    "            lambda row: compute_match_score(\n",
    "                row[\"title\"],\n",
    "                row[\"description\"],\n",
    "                main_skill,\n",
    "                skill_list,\n",
    "                module_skills,\n",
    "                prereq_graph\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        courses[\"difficulty_score\"] = courses.apply(\n",
    "            lambda row: compute_difficulty_scores(row, main_skill, skill_list), axis=1\n",
    "        )\n",
    "\n",
    "        # Normalize scores\n",
    "        for col in [\"match_score\", \"difficulty_score\", \"price\", \"wilson_score\"]:\n",
    "            courses[col] = scaler.fit_transform(courses[[col]])\n",
    "\n",
    "        # Solve ILP\n",
    "        selected_indices = solve_course_selection_pulp(\n",
    "            courses, main_skill, D_ideal, alpha, beta, lambda_, gamma\n",
    "        )\n",
    "\n",
    "        if selected_indices:\n",
    "            selected_courses = course_df.loc[selected_indices].to_dict(orient=\"records\")\n",
    "        else:\n",
    "            # ⛑️ Pick fallback: shortest duration course\n",
    "            fallback = course_df.sort_values(\"duration\").head(1)\n",
    "            selected_courses = fallback.to_dict(orient=\"records\")\n",
    "\n",
    "        result[main_skill] = selected_courses\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "def load_knowledge_graph(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def parse_prerequisite_edges(graph_path, input_skill_set):\n",
    "    knowledge_graph = load_knowledge_graph(graph_path)\n",
    "    prereq_edges = []\n",
    "    for rel in knowledge_graph.get(\"relationships\", []):\n",
    "        if rel[\"relationship\"] == \"prerequisite\":\n",
    "            src, tgt = rel[\"source\"].lower(), rel[\"target\"].lower()\n",
    "            if src in input_skill_set and tgt in input_skill_set:\n",
    "                prereq_edges.append((src, tgt, rel[\"weight\"]))\n",
    "    return prereq_edges\n",
    "\n",
    "\n",
    "def parse_association_weights(graph_path, input_skill_set):\n",
    "    knowledge_graph = load_knowledge_graph(graph_path)\n",
    "    association_scores = {}\n",
    "    for rel in knowledge_graph.get(\"relationships\", []):\n",
    "        if rel[\"relationship\"] == \"association\":\n",
    "            src, tgt = rel[\"source\"].lower(), rel[\"target\"].lower()\n",
    "            if src in input_skill_set and tgt in input_skill_set:\n",
    "                weight = rel.get(\"weight\", 0)\n",
    "                association_scores[(src, tgt)] = weight\n",
    "                association_scores[(tgt, src)] = weight\n",
    "    return association_scores\n",
    "\n",
    "\n",
    "def build_prereq_graph_from_edges(prereq_edges):\n",
    "    G = nx.DiGraph()\n",
    "    for src, tgt, weight in prereq_edges:\n",
    "        G.add_edge(src, tgt, weight=weight)\n",
    "    return G\n",
    "\n",
    "\n",
    "def break_cycles(graph):\n",
    "    while True:\n",
    "        try:\n",
    "            cycle = nx.find_cycle(graph, orientation=\"original\")\n",
    "            weakest = min(cycle, key=lambda e: graph.get_edge_data(e[0], e[1]).get(\"weight\", 1.0))\n",
    "            graph.remove_edge(weakest[0], weakest[1])\n",
    "        except nx.exception.NetworkXNoCycle:\n",
    "            break\n",
    "\n",
    "\n",
    "def topological_sort_with_priorities(prereq_graph, input_skill_set):\n",
    "    break_cycles(prereq_graph)\n",
    "\n",
    "    try:\n",
    "        topo_order = list(nx.topological_sort(prereq_graph))\n",
    "\n",
    "        skill_scores = {}\n",
    "        for skill in topo_order:\n",
    "            incoming = list(prereq_graph.in_edges(skill, data=True))\n",
    "            prereq_weight = sum(data['weight'] for _, _, data in incoming) / len(incoming) if incoming else 0\n",
    "            skill_scores[skill] = prereq_weight\n",
    "\n",
    "        refined_order = sorted(topo_order, key=lambda s: skill_scores[s])\n",
    "\n",
    "        for skill in input_skill_set:\n",
    "            if skill not in refined_order:\n",
    "                refined_order.append(skill)\n",
    "\n",
    "        return refined_order\n",
    "\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        return list(prereq_graph.nodes())\n",
    "\n",
    "\n",
    "def group_skills_by_association(topo_order, association_scores, input_skills, prereq_graph, threshold=0.3):\n",
    "    groups = []\n",
    "    skill_to_module = {}\n",
    "    prereq_map = defaultdict(set)\n",
    "\n",
    "    if not isinstance(prereq_graph, nx.Graph):\n",
    "        prereq_graph = build_prereq_graph_from_edges(prereq_graph)\n",
    "\n",
    "    for src, tgt in prereq_graph.edges():\n",
    "        prereq_map[tgt].add(src)\n",
    "\n",
    "    for skill in topo_order:\n",
    "        max_prereq_module = max(\n",
    "            [skill_to_module[pre] for pre in prereq_map[skill] if pre in skill_to_module],\n",
    "            default=-1\n",
    "        )\n",
    "\n",
    "        best_group = None\n",
    "        best_score = 0\n",
    "\n",
    "        for idx in range(max_prereq_module + 1, len(groups)):\n",
    "            group = groups[idx]\n",
    "            if len(group) >= 3:\n",
    "                continue\n",
    "            score = sum(association_scores.get((skill, other), 0) for other in group)\n",
    "            avg_score = score / len(group) if group else 0\n",
    "            if avg_score >= threshold:\n",
    "                best_group = idx\n",
    "                best_score = avg_score\n",
    "                break\n",
    "\n",
    "        if best_group is not None:\n",
    "            groups[best_group].append(skill)\n",
    "            skill_to_module[skill] = best_group\n",
    "        else:\n",
    "            groups.append([skill])\n",
    "            skill_to_module[skill] = len(groups) - 1\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def assign_module_durations(skill_groups, input_skills, total_hours):\n",
    "    skill_focus_map = {s[0].lower(): s[1] for s in input_skills}\n",
    "    module_durations = []\n",
    "\n",
    "    for group in skill_groups:\n",
    "        durations = [round(skill_focus_map.get(skill, 0) * total_hours, 1) for skill in group]\n",
    "        module_durations.append(durations)\n",
    "\n",
    "    modules = []\n",
    "    for i, (skills, durations) in enumerate(zip(skill_groups, module_durations), 1):\n",
    "        modules.append({\n",
    "            \"module\": i,\n",
    "            \"skills\": skills,\n",
    "            \"duration\": durations\n",
    "        })\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched job domain: Data Scientist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'module': 1, 'skills': ['python'], 'duration': [60.0]},\n",
       " {'module': 2, 'skills': ['machine learning'], 'duration': [20.0]},\n",
       " {'module': 3, 'skills': ['nlp'], 'duration': [18.0]},\n",
       " {'module': 4,\n",
       "  'skills': ['pytorch', 'deep learning'],\n",
       "  'duration': [10.0, 10.0]},\n",
       " {'module': 5, 'skills': ['data analysis'], 'duration': [30.0]},\n",
       " {'module': 6, 'skills': ['pandas', 'git'], 'duration': [10.0, 2.0]},\n",
       " {'module': 7, 'skills': ['sql'], 'duration': [20.0]},\n",
       " {'module': 8, 'skills': ['excel'], 'duration': [20.0]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 1: Load Job Domains ===\n",
    "with open(\"../job_domain_skills.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    job_domains = json.load(f)\n",
    "\n",
    "# === Step 2: User Inputs ===\n",
    "input_skills = [\n",
    "    [\"python\", 0.3, 0.6], \n",
    "    [\"machine learning\", 0.1, 0.3],\n",
    "    [\"pandas\", 0.05, 0.1],\n",
    "    [\"sql\", 0.1, 0.3],\n",
    "    [\"git\", 0.01, 0.1],\n",
    "    [\"data analysis\", 0.15, 0.4],\n",
    "    [\"nlp\", 0.09, 0],\n",
    "    [\"deep learning\", 0.05, 0.1],\n",
    "    [\"excel\", 0.1, 0.4], \n",
    "    [\"pytorch\", 0.05, 0]\n",
    "]\n",
    "input_job_title = \"Data Analyst\"\n",
    "total_weeks = 10\n",
    "weekly_hours = 20\n",
    "total_hours = total_weeks * weekly_hours\n",
    "\n",
    "# === Step 3: Match Domain ===\n",
    "best_match = matchJobDomain(embedding_model, job_domains, input_job_title, input_skills)\n",
    "matched_domain = best_match[\"matched_domain\"]\n",
    "print(\"Matched job domain:\", matched_domain)\n",
    "\n",
    "# === Step 4: Graph Paths & Inputs ===\n",
    "graph_path = f\"./updated_skill_graph/{matched_domain}.json\"\n",
    "input_skill_set = set(s[0].lower() for s in input_skills)\n",
    "\n",
    "# === Step 5: Extract Graph Info ===\n",
    "prereq_edges = parse_prerequisite_edges(graph_path, input_skill_set)\n",
    "assoc_scores = parse_association_weights(graph_path, input_skill_set)\n",
    "\n",
    "# === Step 6: Build Prereq Graph & Topo Sort ===\n",
    "prereq_graph = build_prereq_graph_from_edges(prereq_edges)\n",
    "ordered_skills = topological_sort_with_priorities(prereq_graph, input_skill_set)\n",
    "\n",
    "# === Step 7: Group & Allocate Time ===\n",
    "skill_groups = group_skills_by_association(\n",
    "    ordered_skills, assoc_scores, input_skills, prereq_graph, threshold=0.2\n",
    ")\n",
    "final_modules = assign_module_durations(skill_groups, input_skills, total_hours)\n",
    "\n",
    "final_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Coursera and Udemy course df\n",
    "coursera_file_path = f\"../cleaned_df/coursera_cleaned_df/{matched_domain}.csv\"\n",
    "udemy_file_path = f\"../cleaned_df/udemy_cleaned_df/{matched_domain}.csv\"\n",
    "new_column_order = [\n",
    "    \"skill\", \"link\", \"image_link\", \"partner\", \"title\", \"description\", \"rating\",\n",
    "    \"num_review\", \"duration\", \"price\", \"difficulty_numeric\", \"course_type\", \"wilson_score\", \n",
    "]\n",
    "# Read CSV files into Pandas DataFrames\n",
    "coursera_df = pd.read_csv(coursera_file_path)[new_column_order]\n",
    "udemy_df = pd.read_csv(udemy_file_path)[new_column_order]\n",
    "course_df = pd.concat([coursera_df, udemy_df], axis=0).reset_index()\n",
    "course_df[\"difficulty\"] = course_df[\"difficulty_numeric\"]\n",
    "course_df[\"price\"] = round(course_df[\"price\"], 0) + 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions = suggest_courses(\n",
    "    embedding_model=embedding_model, \n",
    "    course_df=course_df, \n",
    "    skill_list=input_skills, \n",
    "    total_weeks=total_weeks, \n",
    "    weekly_hours=weekly_hours, \n",
    "    job_title=matched_domain, \n",
    "    module_skills=final_modules,\n",
    "    prereq_graph=prereq_graph,\n",
    "    portion=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Cannot fit required hours into day schedule",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 172\u001b[39m\n\u001b[32m    168\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m final\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot fit required hours into day schedule\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[43mcreate_time_blocks_for_day\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mcreate_time_blocks_for_day\u001b[39m\u001b[34m(required_hours)\u001b[39m\n\u001b[32m    167\u001b[39m                 i += \u001b[32m1\u001b[39m\n\u001b[32m    168\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m final\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot fit required hours into day schedule\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Cannot fit required hours into day schedule"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, time\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "USER_ID = 1\n",
    "START_DATE = datetime.strptime(\"2025-04-10\", \"%Y-%m-%d\")\n",
    "WEEKLY_HOURS = 20\n",
    "LEARNING_DAYS = {\n",
    "    \"Monday\": True,\n",
    "    \"Tuesday\": True,\n",
    "    \"Wednesday\": True,\n",
    "    \"Thursday\": False,\n",
    "    \"Friday\": False,\n",
    "    \"Saturday\": True,\n",
    "    \"Sunday\": True,\n",
    "}\n",
    "\n",
    "LUNCH_BREAK = (time(11, 0), time(12, 0))\n",
    "DINNER_BREAK = (time(17, 0), time(18, 0))\n",
    "BREAK_OPTIONS = [timedelta(minutes=15), timedelta(minutes=30)]\n",
    "MIN_CHUNK = 1.0  # in hours\n",
    "\n",
    "# === DAILY HOURS DISTRIBUTOR ===\n",
    "def distribute_weekly_hours(weekly_hours, learning_days):\n",
    "    total_days = sum(learning_days.values())\n",
    "    weekend_days = int(learning_days.get(\"Saturday\", False)) + int(learning_days.get(\"Sunday\", False))\n",
    "    weekday_days = total_days - weekend_days\n",
    "\n",
    "    if weekend_days == 0:\n",
    "        base_hour = weekly_hours / total_days\n",
    "        return {day: round(base_hour * 2) / 2 for day, v in learning_days.items() if v}\n",
    "\n",
    "    weekday_hours = weekly_hours - weekend_days * 1\n",
    "    if weekday_days * 8 < weekday_hours:\n",
    "        weekday_hours = weekday_days * 8\n",
    "        weekend_hours = weekly_hours - weekday_hours\n",
    "        base_weekend = weekend_hours / weekend_days\n",
    "    else:\n",
    "        base_weekend = 1\n",
    "\n",
    "    base_weekday = weekday_hours / weekday_days\n",
    "    all_days = {}\n",
    "    for day, v in learning_days.items():\n",
    "        if not v:\n",
    "            continue\n",
    "        if day in [\"Saturday\", \"Sunday\"]:\n",
    "            all_days[day] = round(base_weekend * 2) / 2\n",
    "        else:\n",
    "            all_days[day] = round(base_weekday * 2) / 2\n",
    "\n",
    "    return all_days\n",
    "\n",
    "# === HELPER: Round-robin hour splitting ===\n",
    "def split_hours_round_robin(total_hours, num_blocks):\n",
    "    blocks = [0.0] * num_blocks\n",
    "    i = 0\n",
    "    while total_hours >= 0.5:\n",
    "        blocks[i] += 0.5\n",
    "        total_hours -= 0.5\n",
    "        i = (i + 1) % num_blocks\n",
    "    return blocks\n",
    "\n",
    "from datetime import datetime, timedelta, time\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# === CONFIG ===\n",
    "start_hour = 8\n",
    "end_hour = 21\n",
    "slot_duration = 15  # minutes\n",
    "slots_per_hour = 60 // slot_duration\n",
    "total_slots = (end_hour - start_hour) * slots_per_hour\n",
    "daily_hours = 6\n",
    "target_minutes = daily_hours * 60\n",
    "\n",
    "# === Time slots ===\n",
    "time_slots = []\n",
    "current_minutes = start_hour * 60\n",
    "for i in range(total_slots):\n",
    "    start = current_minutes\n",
    "    end = start + slot_duration\n",
    "    midpoint = (start + end) // 2\n",
    "    time_slots.append({\n",
    "        \"index\": i,\n",
    "        \"start_min\": start,\n",
    "        \"end_min\": end,\n",
    "        \"mid_min\": midpoint,\n",
    "        \"comfort\": -abs(midpoint - 12 * 60)  # closer to noon = better\n",
    "    })\n",
    "    current_minutes += slot_duration\n",
    "\n",
    "# === Forbidden time ranges (11–12 and 17–18)\n",
    "forbidden_ranges = [(11*60, 12*60), (17*60, 18*60)]\n",
    "forbidden_slots = [s[\"index\"] for s in time_slots if any(start <= s[\"mid_min\"] < end for start, end in forbidden_ranges)]\n",
    "\n",
    "# === ILP Model ===\n",
    "model = LpProblem(\"DailyBlockComfort\", LpMaximize)\n",
    "x = {s[\"index\"]: LpVariable(f\"x_{s['index']}\", cat=LpBinary) for s in time_slots}\n",
    "\n",
    "# Objective: maximize comfort\n",
    "model += lpSum(x[s[\"index\"]] * s[\"comfort\"] for s in time_slots)\n",
    "\n",
    "# Learning time constraint\n",
    "model += lpSum(x[s[\"index\"]] * slot_duration for s in time_slots) == target_minutes\n",
    "\n",
    "# No learning during forbidden slots\n",
    "for idx in forbidden_slots:\n",
    "    model += x[idx] == 0\n",
    "\n",
    "# At least 4, at most 12 consecutive slots with at least 1 break slot between\n",
    "# (soft version here — can be made stricter with auxiliary variables)\n",
    "\n",
    "# Solve\n",
    "model.solve(PULP_CBC_CMD(msg=0))\n",
    "\n",
    "# Output selected blocks\n",
    "selected_blocks = []\n",
    "for s in time_slots:\n",
    "    if x[s[\"index\"]].varValue == 1:\n",
    "        st = time(s[\"start_min\"] // 60, s[\"start_min\"] % 60)\n",
    "        et = time(s[\"end_min\"] // 60, s[\"end_min\"] % 60)\n",
    "        selected_blocks.append((st, et))\n",
    "\n",
    "for block in selected_blocks:\n",
    "    print(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Monday': 6.0,\n",
       " 'Tuesday': 6.0,\n",
       " 'Wednesday': 6.0,\n",
       " 'Saturday': 1.0,\n",
       " 'Sunday': 1.0}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_hours_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
